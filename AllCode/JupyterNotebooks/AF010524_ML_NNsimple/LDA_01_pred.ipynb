{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a312dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import zscore\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa950908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_with_gaussian(data, sigma=2):\n",
    "    return gaussian_filter1d(data, sigma=sigma, axis=1)\n",
    "\n",
    "def preprocess(data):\n",
    "    stds = np.std(data[:, :], axis=0)\n",
    "    non_constant_cols = stds.astype(float) > 1e-6    #finds the time points where std is not 0\n",
    "    const_cols = stds.astype(float) <= 1e-6    #finds the time points where std is 0\n",
    "\n",
    "    z = np.zeros_like(data[:, :])   #creates an array of zeros with the same shape as the data\n",
    "    z[:, non_constant_cols] = zscore(data[:, non_constant_cols], axis=0)  #in the columns where std is not 0, zscores the data\n",
    "    z[:, const_cols] = np.mean(data[:, const_cols], axis=0)\n",
    "\n",
    " \n",
    "    if np.isnan(z).any():\n",
    "        raise ValueError(\"Data contains NaN values after normalization.\")\n",
    "\n",
    "    return z\n",
    "\n",
    "def load_and_split(file_numbers):\n",
    "    l_al = []\n",
    "    l_ar = []\n",
    "    r_al = []\n",
    "    r_ar = []\n",
    "    total_time = time.time()\n",
    "    for file_number in  file_numbers:\n",
    "        file_total = time.time()\n",
    "        file_path = f'C:/Users/joshu/PartIIIProject/RSNNdale_attention_{file_number}_attention_test'\n",
    "        load_data_start_time = time.time()\n",
    "        data = pickle.load(open(file_path, 'rb'))\n",
    "        elapsed_time = time.time() - load_data_start_time\n",
    "        print(f\"Dataset {file_number} loaded in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        attend_01 = data['attend'][0]\n",
    "        omitted = data['omit'][0]\n",
    "        relevant = np.where(omitted ==0)[0]\n",
    "\n",
    "        left_input_SP = data['SP'][0][0][relevant]\n",
    "        right_input_SP = data['SP'][0][1][relevant]\n",
    "\n",
    "        sigma = 2\n",
    "        left_sm = smooth_with_gaussian(left_input_SP, sigma=sigma) \n",
    "        right_sm = smooth_with_gaussian(right_input_SP, sigma=sigma) \n",
    "\n",
    "        num_trials, num_samples, num_neurons = left_input_SP.shape\n",
    "        num_neurons_attention = 80\n",
    "\n",
    "        for j in range(0, num_trials):\n",
    "            for i in range(0, num_neurons):\n",
    "                count_left = np.count_nonzero(left_input_SP[j, :, i] == 1)\n",
    "                if count_left > 0:\n",
    "                    left_sm[j, :, i] /= count_left\n",
    "                count_right = np.count_nonzero(right_input_SP[j, :, i] == 1)\n",
    "                if count_right > 0:\n",
    "                    right_sm[j, :, i] /= count_right\n",
    "\n",
    "        left_input_SP = np.sum(left_sm, axis=2)\n",
    "        right_input_SP = np.sum(right_sm, axis=2)\n",
    "\n",
    "        left_input_SP = preprocess(left_input_SP)\n",
    "        right_input_SP = preprocess(right_input_SP)\n",
    "\n",
    "\n",
    "        #preprocess here now that we have traces of all of the relavant trials\n",
    "        left_indices_agg = np.where((omitted ==0) & (attend_01 == 0))[0]  #indices of agg where left\n",
    "        _, left_indices, _ = np.intersect1d(relevant, left_indices_agg, return_indices = True)   #indices for relevant processed data where attention left\n",
    "        right_indices_agg = np.where((omitted ==0) & (attend_01 == 1))[0]\n",
    "        _, right_indices, _ = np.intersect1d(relevant, right_indices_agg, return_indices = True)\n",
    "\n",
    "        #splitting left and right\n",
    "        l_al.append(left_input_SP[left_indices, 100:350])\n",
    "        r_al.append(right_input_SP[left_indices, 100:350])\n",
    "\n",
    "        l_ar.append(left_input_SP[right_indices, 100:350])\n",
    "        r_ar.append(right_input_SP[right_indices, 100:350])\n",
    "        \n",
    "\n",
    "        print(f\"Dataset {file_number} processed in {time.time() - file_total:.2f} seconds\")\n",
    "    print(f\"All datasets processed in {time.time() - total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72cf032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 loaded in 51.02 seconds\n",
      "Dataset 1 processed in 65.29 seconds\n",
      "Dataset 2 loaded in 23.90 seconds\n",
      "Dataset 2 processed in 36.17 seconds\n",
      "Dataset 4 loaded in 15.26 seconds\n",
      "Dataset 4 processed in 31.66 seconds\n",
      "Dataset 8 loaded in 18.10 seconds\n",
      "Dataset 8 processed in 28.43 seconds\n",
      "All datasets processed in 161.70 seconds\n"
     ]
    }
   ],
   "source": [
    "load_and_split([1, 2, 4, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1798f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_al_flat = np.concatenate(l_al, axis=0)\n",
    "l_ar_flat = np.concatenate(l_ar, axis=0)\n",
    "r_al_flat = np.concatenate(r_al, axis=0)\n",
    "r_ar_flat = np.concatenate(r_ar, axis=0)\n",
    "\n",
    "X_in  = np.concatenate([l_al_flat, r_ar_flat], axis=0)\n",
    "# Out‐condition: unattended\n",
    "X_out = np.concatenate([r_al_flat, l_ar_flat], axis=0)\n",
    "\n",
    "y_in  = np.concatenate([\n",
    "    np.zeros(l_al_flat.shape[0], dtype=int),\n",
    "    np.ones( r_ar_flat.shape[0], dtype=int)\n",
    "])\n",
    "# same labels for out\n",
    "y_out = np.concatenate([\n",
    "    np.zeros(r_al_flat.shape[0], dtype=int),\n",
    "    np.ones( l_ar_flat.shape[0], dtype=int)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a85ecfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-condition LDA →  Acc: 0.460,  AUC: 0.457\n",
      "Out-condition LDA →  Acc: 0.473,  AUC: 0.455\n"
     ]
    }
   ],
   "source": [
    "def compare_lda_in_out(X_in, y_in, X_out, y_out, test_size=0.2, random_state=42):\n",
    "    results = {}\n",
    "    for name, (X, y) in [('In', (X_in, y_in)), ('Out', (X_out, y_out))]:\n",
    "        # flatten is already done; split\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X, y, test_size=test_size, \n",
    "            random_state=random_state, stratify=y\n",
    "        )\n",
    "        # scale\n",
    "        scaler = StandardScaler().fit(Xtr)\n",
    "        Xtr_s = scaler.transform(Xtr)\n",
    "        Xte_s = scaler.transform(Xte)\n",
    "        # fit LDA\n",
    "        lda = LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 'auto').fit(Xtr_s, ytr)\n",
    "        # predict\n",
    "        yhat   = lda.predict(Xte_s)\n",
    "        yprob  = lda.predict_proba(Xte_s)[:,1]\n",
    "        # metrics\n",
    "        acc  = accuracy_score(yte, yhat)\n",
    "        auc  = roc_auc_score(yte, yprob)\n",
    "        results[name] = {'accuracy': acc, 'roc_auc': auc}\n",
    "        print(f\"{name}-condition LDA →  Acc: {acc:.3f},  AUC: {auc:.3f}\")\n",
    "    return results\n",
    "\n",
    "# Example:\n",
    "results = compare_lda_in_out(X_in, y_in, X_out, y_out)\n",
    "# Example usage:\n",
    "# X: shape (n_trials, 250, 80)\n",
    "# y: binary labels array of shape (n_trials,)\n",
    "# lda_model, scaler, lda_metrics = run_lda(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab6dad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-condition LDA → Acc: 0.465, AUC: 0.469, best PCA=100, shrinkage=0.5\n",
      "Out-condition LDA → Acc: 0.488, AUC: 0.478, best PCA=50, shrinkage=0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def compare_lda_in_out(X_in, y_in, X_out, y_out,\n",
    "                       test_size=0.2, random_state=42,\n",
    "                       pca_components=[50, 100, 200],\n",
    "                       shrinkages=['auto', 0.1, 0.5]):\n",
    "    \"\"\"\n",
    "    Compare LDA performance on attended (in) vs. unattended (out) channels,\n",
    "    using PCA + shrinkage LDA.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_in, X_out: arrays of shape (n_trials, n_features) or (n_trials, n_times, n_neurons)\n",
    "    - y_in, y_out: binary labels arrays of shape (n_trials,)\n",
    "    - pca_components: list of PCA n_components to grid-search\n",
    "    - shrinkages: list of shrinkage values for LDA ('auto' or float)\n",
    "    \n",
    "    Returns:\n",
    "    - results: dict with best estimators and metrics for 'In' and 'Out'\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for name, (X, y) in [('In', (X_in, y_in)), ('Out', (X_out, y_out))]:\n",
    "        # If 3D inputs, flatten to (n_trials, n_times * n_neurons)\n",
    "        if X.ndim == 3:\n",
    "            n_trials = X.shape[0]\n",
    "            X_flat = X.reshape(n_trials, -1)\n",
    "        else:\n",
    "            X_flat = X\n",
    "\n",
    "        # Split\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_flat, y, test_size=test_size,\n",
    "            random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Pipeline: scale -> PCA -> LDA with shrinkage\n",
    "        pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca',    PCA()),\n",
    "            ('lda',    LinearDiscriminantAnalysis(solver='lsqr'))\n",
    "        ])\n",
    "        \n",
    "        param_grid = {\n",
    "            'pca__n_components': pca_components,\n",
    "            'lda__shrinkage':    shrinkages\n",
    "        }\n",
    "        \n",
    "        grid = GridSearchCV(pipe, param_grid, cv=4,\n",
    "                            scoring='roc_auc', n_jobs=-1)\n",
    "        grid.fit(Xtr, ytr)\n",
    "        \n",
    "        # Evaluate\n",
    "        yprob = grid.predict_proba(Xte)[:,1]\n",
    "        yhat  = grid.predict(Xte)\n",
    "        acc   = accuracy_score(yte, yhat)\n",
    "        auc   = roc_auc_score(yte, yprob)\n",
    "        \n",
    "        print(f\"{name}-condition LDA → Acc: {acc:.3f}, AUC: {auc:.3f}, \"\n",
    "              f\"best PCA={grid.best_params_['pca__n_components']}, \"\n",
    "              f\"shrinkage={grid.best_params_['lda__shrinkage']}\")\n",
    "        \n",
    "        results[name] = {\n",
    "            'best_estimator': grid.best_estimator_,\n",
    "            'accuracy': acc,\n",
    "            'roc_auc': auc,\n",
    "            'best_params': grid.best_params_\n",
    "        }\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "results = compare_lda_in_out(X_in, y_in, X_out, y_out)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
