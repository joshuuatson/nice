{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from nice.algorithms.connectivity_AT import *\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_numbers = [1, 2, 3]\n",
    "structure = {f'dataset_{file_number}': {\n",
    "    'left_attleft': [],\n",
    "    'right_attleft': [],\n",
    "    'left_attright': [],\n",
    "    'right_attright': []\n",
    "} for file_number in file_numbers}\n",
    "\n",
    "def preprocess(data):\n",
    "    #data shape: (trials, time, neurons)\n",
    "    trials, time, num_neurons = data.shape\n",
    "    z = np.zeros_like(data)\n",
    "\n",
    "    for neuron in range(num_neurons):\n",
    "        for t in range(time):\n",
    "            vals = data[:, t, neuron]  #this is all the data for given neuron at given time point\n",
    "            std = np.std(vals)\n",
    "            if std > 1e-6:\n",
    "                z[:, t, neuron] = (vals - np.mean(vals)) / std    #zscore if there is a stdev\n",
    "            else:\n",
    "                z[:, t, neuron] = np.median(vals)   #returnt the median value otherwise  (median rather than mean in case of large variability in a singel point)\n",
    "\n",
    "    if np.isnan(z).any():\n",
    "        raise ValueError(\"Data contains NaN values after normalization.\")\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def get_data(file_number):\n",
    "    file_path = f'C:/Users/joshu/PartIIIProject/RSNNdale_attention_{file_number}_attention_test'\n",
    "    data = pickle.load(open(file_path, 'rb'))\n",
    "  \n",
    "    label_left = data['label_left'][0]\n",
    "    label_right = data['label_right'][0]  #for class of input\n",
    "    attend_01 = data['attend'][0]\n",
    "    omitted = data['omit'][0]\n",
    " \n",
    "    #add in here at some point left != right and see if it makes a difference\n",
    "    left_indices_agg = np.where((omitted == 0) & (attend_01 == 0) & (label_left != label_right))[0]  #indices of agg where left\n",
    "    right_indices_agg = np.where((omitted == 0) & (attend_01 == 1) & (label_left != label_right))[0]\n",
    "  \n",
    "    left_attleft = data['SP'][0][0][left_indices_agg, 100:350, :]  #left input spikes\n",
    "    print(left_attleft.shape)\n",
    "    plt.plot(left_attleft[0, :, 0])\n",
    "    plt.title('left_attleft')\n",
    "    plt.show()\n",
    "\n",
    "    right_attleft = data['SP'][0][1][left_indices_agg, 100:350, :]\n",
    "    att_attleft = data['SP'][0][2][left_indices_agg, 100:350, :]\n",
    "\n",
    "    left_attright = data['SP'][0][0][right_indices_agg, 100:350, :]  #right input spikes\n",
    "    right_attright = data['SP'][0][1][right_indices_agg, 100:350, :]\n",
    "    att_attright = data['SP'][0][2][right_indices_agg, 100:350, :]  \n",
    "    \n",
    "\n",
    "    return left_attleft, left_attright, right_attleft, right_attright, att_attleft, att_attright\n",
    "##e.g. dataset 1 gives left_attleft of shape (469 trials, 500 time points, 160 spikes)\n",
    "\n",
    "l_al = []\n",
    "l_ar = []\n",
    "r_al = []\n",
    "r_ar = []\n",
    "a_al = []\n",
    "a_ar = []\n",
    "\n",
    "def collect_data(file_numbers):\n",
    "    for file_number in file_numbers:\n",
    "        left_attleft, left_attright, right_attleft, right_attright, att_attleft, att_attright = get_data(file_number)\n",
    "        l_al.append(left_attleft)\n",
    "        l_ar.append(left_attright)\n",
    "        r_al.append(right_attleft)\n",
    "        r_ar.append(right_attright)\n",
    "        a_al.append(att_attleft)\n",
    "        a_ar.append(att_attright)\n",
    "       \n",
    "    return l_al, l_ar, r_al, r_ar, a_al, a_ar\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_al, _, r_al, _, a_al, _ = collect_data(file_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b15be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_neuron_firings(data):\n",
    "    return np.sum(data, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perhaps prep the data here - \n",
    "#will calculate mean and std for each neuron across trials \n",
    "#then will zscore per neuron\n",
    "#perhaps rescale to 0-1 for each neuron across trials\n",
    "#if i end up binning the data, perhaps zscore each bin\n",
    "#should compute mean and std only using the training data, then zscore both training and test \n",
    "#using these training stats\n",
    "\n",
    "def smooth_data(data, sigma=1):\n",
    "    return gaussian_filter1d(data, sigma=sigma, axis=1)  #Gaussian filter along the time axis\n",
    "\n",
    "def normalize_inputs(x_train, x_test, x_val):\n",
    "    z_train = np.zeros_like(x_train)\n",
    "    z_test = np.zeros_like(x_test)\n",
    "    z_val = np.zeros_like(x_val)\n",
    "\n",
    "    x_train = smooth_data(x_train, sigma=1)\n",
    "    x_test = smooth_data(x_test, sigma=1)\n",
    "    x_val = smooth_data(x_val, sigma=1)\n",
    "\n",
    "    mean = np.mean(x_train, axis=0)  #(1, time, neurons)\n",
    "    std = np.std(x_train, axis=0)\n",
    "\n",
    "    #avoids divide-by-zero\n",
    "    std[std < 1e-6] = 1.0\n",
    "\n",
    "    z_train = (x_train - mean) / std\n",
    "    z_test  = (x_test - mean) / std\n",
    "    z_val   = (x_val - mean) / std\n",
    "\n",
    "    if np.isnan(z_train).any():\n",
    "        raise ValueError(\"Data contains NaN values after normalization.\")\n",
    "\n",
    "    return (\n",
    "        torch.tensor(z_train, dtype=torch.float32),\n",
    "        torch.tensor(z_test, dtype=torch.float32),\n",
    "        torch.tensor(z_val, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_targets(y_train, y_test, y_val):\n",
    "    mean = np.mean(y_train, axis = 0)  #shape (1, 80)\n",
    "    std = np.std(y_train, axis=0)\n",
    "\n",
    "    std[std < 1e-6] = 1.0\n",
    "\n",
    "    y_train_norm = (y_train - mean) / std\n",
    "    y_test_norm  = (y_test - mean) / std\n",
    "    y_val_norm   = (y_val - mean) / std\n",
    "\n",
    "    return (\n",
    "        torch.tensor(y_train_norm, dtype=torch.float32),\n",
    "        torch.tensor(y_test_norm, dtype=torch.float32),\n",
    "        torch.tensor(y_val_norm, dtype=torch.float32),\n",
    "        mean, std\n",
    "    )\n",
    "\n",
    "\n",
    "def prep_data(x, y, batch_size = 16, shuffle = True):\n",
    "    x_raw = np.concatenate(x, axis = 0)\n",
    "    y_raw = np.concatenate(y, axis = 0)\n",
    "    print(x_raw.shape)\n",
    "    print(y_raw.shape)\n",
    "\n",
    "    y = count_neuron_firings(y_raw)  #counts neuron firings for each trial\n",
    "    print(y.shape)\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_raw, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    x_train, x_test, x_validation = normalize_inputs(x_train, x_test, x_validation)\n",
    "    y_train, y_test, y_validation, mean, std = normalize_targets(y_train, y_test, y_validation)\n",
    "\n",
    "    plt.plot(y_train[0, :])\n",
    "    plt.title('y_train')\n",
    "    plt.show()\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True, num_workers=0)\n",
    "    return train_loader, x_test, y_test, x_validation, y_validation, mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data(l_al, a_al, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingAttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_dim = 160, hidden_dim = 128, output_dim = 80, num_layers = 2, dropout = 0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_dim,\n",
    "            hidden_size = hidden_dim, \n",
    "            batch_first = True,\n",
    "            num_layers = num_layers,\n",
    "            dropout = dropout if num_layers > 1 else 0.0,  #dropout only applies if num_layers > 1\n",
    "\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)  #dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)  #(batch, time, hidden)\n",
    "        pooled = output.mean(dim=1)  #mean over time â†’ shape: (batch, hidden)\n",
    "        dropped = self.dropout(pooled)\n",
    "        out = self.fc(dropped)  #shape: (batch, 80)\n",
    "        return out\n",
    "    \n",
    "class SpikingAttentionNN(nn.Module):\n",
    "    def __init__(self, input_dim = 160, output_dim = 80, hidden_dim = 128, num_layers = 2, dropout = 0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ad282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, x_val, y_val, \n",
    "           criterion, epochs=1000, clip_value = 1.0, \n",
    "           lr = 0.001, patience = 20):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)  #gradient clipping\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(x_val)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "            val_loss_item = val_loss.item()\n",
    "            validation_loss_history.append(val_loss.item())\n",
    "\n",
    "        #early stopping\n",
    "        if val_loss_item < best_val_loss:\n",
    "            best_val_loss = val_loss_item\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} with validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            clear_output(wait=True)  #clears the Jupyter output cell\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            plt.plot(loss_history, label = 'Training Loss')\n",
    "            plt.plot(validation_loss_history, label = 'Validation Loss')\n",
    "            plt.legend()\n",
    "            plt.xlim(0, epochs)\n",
    "            plt.ylim(0, 0.7)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Losses')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return loss_history, validation_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_ground_truth(model_left, x_data_left, y_data_left, model_right, x_data_right, y_data_right, num_examples=5, width = 16):\n",
    "    \"\"\"\n",
    "    model     : your trained model\n",
    "    x_data    : (n_trials, time, input_dim) or subset for val/test set\n",
    "    y_data    : (n_trials, output_dim) ground truth\n",
    "    num_examples : how many random trials to plot\n",
    "    \"\"\"\n",
    "    model_left.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_left = model_left(x_data_left)  #(n_trials, 80)\n",
    "    \n",
    "    y_pred_left_np = y_pred_left.detach().cpu().numpy()\n",
    "    y_data_left_np = y_data_left.detach().cpu().numpy() if isinstance(y_data_left, torch.Tensor) else y_data_left\n",
    "\n",
    "    n_trials = y_pred_left_np.shape[0]\n",
    "\n",
    "    model_right.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_right = model_right(x_data_right)\n",
    "\n",
    "    y_pred_right_np = y_pred_right.detach().cpu().numpy()\n",
    "    y_data_right_np = y_data_right.detach().cpu().numpy() if isinstance(y_data_right, torch.Tensor) else y_data_right\n",
    "    \n",
    "    #randomly choose some trials to visualize\n",
    "    trials_to_plot = np.random.choice(n_trials, size=num_examples, replace=False)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, num_examples, figsize=(width, 7))\n",
    "    \n",
    "    if num_examples == 1:\n",
    "        ax = np.array(ax).reshape(2, 1)\n",
    "        \n",
    "    for i, trial_idx in enumerate(trials_to_plot):\n",
    "        ax[0, i].plot(y_data_left_np[trial_idx], 'ko-', label='Ground Truth')\n",
    "        ax[0, i].plot(y_pred_left_np[trial_idx], 'ro-', label='Prediction')\n",
    "        ax[0, i].set_title(f'Left Model - Trial {trial_idx}')\n",
    "        ax[0, i].set_xlabel('Neuron Index')\n",
    "        ax[0, i].set_ylabel('Firing Rate')\n",
    "        ax[0, i].legend()\n",
    "        ax[0, i].grid(True)\n",
    "\n",
    "        ax[1, i].plot(y_data_right_np[trial_idx], 'ko-', label='Ground Truth')\n",
    "        ax[1, i].plot(y_pred_right_np[trial_idx], 'ro-', label='Prediction')\n",
    "        ax[1, i].set_title(f'Right Model - Trial {trial_idx}')\n",
    "        ax[1, i].set_xlabel('Neuron Index')\n",
    "        ax[1, i].set_ylabel('Firing Rate')\n",
    "        ax[1, i].legend()\n",
    "        ax[1, i].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "train_loader_left, x_test_left, y_test_left, x_validation_left, y_validation_left, _, _ = prep_data(l_al, a_al, batch_size = 32, shuffle = True)\n",
    "\n",
    "n_neurons_in = x_test_left.shape[2]\n",
    "print('n_neurons:', n_neurons_in)\n",
    "\n",
    "model_left = SpikingAttentionLSTM(input_dim = n_neurons_in, hidden_dim = 32, output_dim = 80, num_layers = 2, dropout = 0.3)\n",
    "\n",
    "criterion =  nn.MSELoss()\n",
    "loss_history_left, validation_loss_history_left = train(model_left, train_loader_left, x_validation_left, y_validation_left,\n",
    "                 criterion, epochs = 150, lr = 0.005, patience = 20)\n",
    "\n",
    "model_left.eval()\n",
    "y_pred_trained_l = model_left(x_test_left).detach().numpy()\n",
    "print('y_pred shape:', y_pred_trained_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a413a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "train_loader_right, x_test_right, y_test_right, x_validation_right, y_validation_right, _, _ = prep_data(r_al, a_al, batch_size = 32, shuffle = True)\n",
    "\n",
    "n_neurons_in = x_test_right.shape[2]\n",
    "print('n_neurons:', n_neurons_in)\n",
    "\n",
    "model_right = SpikingAttentionLSTM(input_dim = n_neurons_in, hidden_dim = 32, output_dim = 80, num_layers = 2, dropout = 0.3)\n",
    "\n",
    "criterion =  nn.MSELoss()\n",
    "loss_history_right, validation_loss_history_right = train(model_right, train_loader_right, x_validation_right, y_validation_right,\n",
    "                 criterion, epochs = 150, lr = 0.005, patience = 20)\n",
    "\n",
    "model_right.eval()\n",
    "y_pred_trained_r = model_right(x_test_right).detach().numpy()\n",
    "print('y_pred shape:', y_pred_trained_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c300db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_vs_ground_truth(model_left, x_test_left, y_test_right, model_right, x_test_right, y_test_right, num_examples=1, width = 25)\n",
    "#plot_predictions_vs_ground_truth(model_left, x_test_left, y_test_right, model_right, x_test_right, y_test_right, num_examples=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef330ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(y_true, y_pred):\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    flat_y_true = y_true.flatten()\n",
    "    flat_y_pred = y_pred.flatten()\n",
    "    overall_corr = np.corrcoef(flat_y_true, flat_y_pred)[0, 1]\n",
    "\n",
    "    #per-neuron correlation along trials\n",
    "    n_trials, n_neurons = y_true.shape\n",
    "    per_neuron_corr = np.zeros(n_neurons)\n",
    "    for i in range(n_neurons):\n",
    "        # Option 1: Using np.corrcoef:\n",
    "        per_neuron_corr[i] = np.corrcoef(y_true[:, i], y_pred[:, i])[0, 1]\n",
    "        # Option 2: Alternatively, you can use scipy.stats.pearsonr:\n",
    "        # r, _ = pearsonr(y_true[:, i], y_pred[:, i])\n",
    "        # per_neuron_corr[i] = r\n",
    "\n",
    "    return overall_corr, per_neuron_corr\n",
    "\n",
    "\n",
    "overall_corr_left, per_neuron_corr_left = compute_correlations(y_test_left, y_pred_trained_l)\n",
    "overall_corr_right, per_neuron_corr_right = compute_correlations(y_test_right, y_pred_trained_r)\n",
    "\n",
    "print(\"Overall Pearson Correlation:\", overall_corr_left)\n",
    "#print(\"Per-neuron Pearson Correlations:\", per_neuron_corr_left)\n",
    "print(\"Overall Pearson Correlation:\", overall_corr_right)\n",
    "#print(\"Per-neuron Pearson Correlations:\", per_neuron_corr_right)\n",
    "\n",
    "\n",
    "# Optional: A function to visualize per-neuron correlations\n",
    "def plot_neuron_correlations(per_neuron_corr):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(np.arange(len(per_neuron_corr)), per_neuron_corr, color='skyblue')\n",
    "    plt.xlabel('Neuron Index')\n",
    "    plt.ylabel('Pearson Correlation')\n",
    "    plt.title('Correlation per Neuron')\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# To visualize the per-neuron correlations:\n",
    "plot_neuron_correlations(per_neuron_corr_left)\n",
    "plot_neuron_correlations(per_neuron_corr_right)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(-per_neuron_corr_left, bins=12, alpha=0.5, label='Left Correlations (Flipped)', color='orange')\n",
    "plt.hist(-per_neuron_corr_right, bins=12, alpha=0.5, label='Right Correlations (Flipped)', color='blue')\n",
    "plt.xlabel('Flipped Pearson Correlation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Flipped Per-Neuron Correlations')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
